# AI Assistant Configuration

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7

# Google Gemini API Configuration (Free Tier: 15 RPM, 1,500 Requests/Day)
# Get your key at: https://makersuite.google.com/app/apikey
#
# === CHAT MODELS ===
# Gemini Chat Models (General purpose conversations)
#   - gemini-2.5-flash: Latest fast model for quick responses
#   - gemini-2.5-flash-lite: Lightweight version for simple tasks
#   - gemini-3-flash-preview: Preview of next-generation model
#   - gemini-robotics-er-1.5-preview: Specialized for robotics applications
#
# Gemma Chat Models (Always add -it suffix for chat)
#   - gemma-3-27b-it: Largest model, best for complex reasoning
#   - gemma-3-12b-it: Balanced performance and speed
#   - gemma-3-4b-it: Lightweight for simple tasks
#   - gemma-3-2b-it: Ultra-lightweight for quick responses
#   - gemma-3-1b-it: Smallest model, fastest responses
#
# === SPEECH MODELS ===
#   - gemini-2.5-flash-tts: Text-to-speech generation
#
# === AUDIO DIALOG MODELS ===
#   - gemini-2.5-flash-preview-native-audio-dialog: Native audio conversation support
#
# === EMBEDDING MODELS ===
#   - text-embedding-004: Text embeddings for semantic search and similarity
#
# Configuration Notes:
#   - MAX_TOKENS: Maximum output tokens (1-8192 recommended)
#   - TEMPERATURE: 0.0 (deterministic) to 1.0 (creative)
#   - MAX_REQUESTS_PER_MINUTE: Rate limit to prevent quota exhaustion
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash
GEMINI_MAX_TOKENS=8192
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_REQUESTS_PER_MINUTE=15

# Groq API Configuration (Free Tier: Variable limits per model)
# Get your key at: https://console.groq.com/keys
#
# === CHAT COMPLETIONS MODELS ===
# General Purpose Chat Models
#   - allam-2-7b: 30 RPM, 7K/day, 6K tokens/min, 500K tokens/day
#   - groq/compound: 30 RPM, 250/day, 70K tokens/min, unlimited tokens/day
#   - groq/compound-mini: 30 RPM, 250/day, 70K tokens/min, unlimited tokens/day
#   - llama-3.1-8b-instant: 30 RPM, 14.4K/day, 6K tokens/min, 500K tokens/day
#   - llama-3.3-70b-versatile: 30 RPM, 1K/day, 12K tokens/min, 100K tokens/day
#
# Llama 4 Models
#   - meta-llama/llama-4-maverick-17b-128e-instruct: 30 RPM, 1K/day, 6K tokens/min, 500K tokens/day
#   - meta-llama/llama-4-scout-17b-16e-instruct: 30 RPM, 1K/day, 30K tokens/min, 500K tokens/day
#
# Guard & Safety Models
#   - meta-llama/llama-guard-4-12b: 30 RPM, 14.4K/day, 15K tokens/min, 500K tokens/day
#   - meta-llama/llama-prompt-guard-2-22m: 30 RPM, 14.4K/day, 15K tokens/min, 500K tokens/day
#   - meta-llama/llama-prompt-guard-2-86m: 30 RPM, 14.4K/day, 15K tokens/min, 500K tokens/day
#
# Moonshot AI Models
#   - moonshotai/kimi-k2-instruct: 60 RPM, 1K/day, 10K tokens/min, 300K tokens/day
#   - moonshotai/kimi-k2-instruct-0905: 60 RPM, 1K/day, 10K tokens/min, 300K tokens/day
#
# OpenAI OSS Models
#   - openai/gpt-oss-120b: 30 RPM, 1K/day, 8K tokens/min, 200K tokens/day
#   - openai/gpt-oss-20b: 30 RPM, 1K/day, 8K tokens/min, 200K tokens/day
#   - openai/gpt-oss-safeguard-20b: 30 RPM, 1K/day, 8K tokens/min, 200K tokens/day
#
# Qwen Models
#   - qwen/qwen3-32b: 60 RPM, 1K/day, 6K tokens/min, 500K tokens/day
#
# === SPEECH TO TEXT MODELS ===
#   - whisper-large-v3: 20 RPM, 2K/day, 7.2K audio seconds/hour, 28.8K audio seconds/day
#   - whisper-large-v3-turbo: 20 RPM, 2K/day, 7.2K audio seconds/hour, 28.8K audio seconds/day
#
# === TEXT TO SPEECH MODELS ===
#   - canopylabs/orpheus-arabic-saudi: 10 RPM, 100/day, 1.2K tokens/min, 3.6K tokens/day
#   - canopylabs/orpheus-v1-english: 10 RPM, 100/day, 1.2K tokens/min, 3.6K tokens/day
#
# Configuration Notes:
#   - MAX_TOKENS: Maximum output tokens (varies by model, typically 4096-8192)
#   - TEMPERATURE: 0.0 (deterministic) to 1.0 (creative)
#   - MAX_REQUESTS_PER_MINUTE: Rate limit to prevent quota exhaustion
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_MAX_TOKENS=4096
GROQ_TEMPERATURE=0.7
GROQ_MAX_REQUESTS_PER_MINUTE=30

# Mistral AI API Configuration (Free Tier: Moderate limits)
# Get your key at: https://console.mistral.ai/
#
# === CHAT MODELS ===
#   - mistral-small-latest: Small model for quick responses
#   - mistral-medium-latest: Medium model for balanced performance
#   - mistral-large-latest: Large model for complex tasks
#   - codestral-latest: Specialized for code generation
#   - open-mistral-7b: Open-source 7B model
#   - open-mixtral-8x7b: Open-source mixture of experts model
#   - ministral-14b-2512: Lightweight 14B model for efficient inference
#
# Configuration Notes:
#   - MAX_TOKENS: Maximum output tokens (typically 4096-8192)
#   - TEMPERATURE: 0.0 (deterministic) to 1.0 (creative)
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-small-latest
MISTRAL_MAX_TOKENS=4096
MISTRAL_TEMPERATURE=0.7

# Hugging Face API Configuration (Free Tier: Variable, Shared GPU)
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=your_huggingface_token_here
HUGGINGFACE_MODEL=meta-llama/Llama-3-8b
HUGGINGFACE_MAX_TOKENS=2048
HUGGINGFACE_TEMPERATURE=0.7

# Pollinations AI Configuration (Free & Unlimited)
# Get your key at: https://pollinations.ai/
# Note: API key is optional but recommended for higher rate limits and priority access
# Pricing: 1 pollen â‰ˆ very low cost (see model-specific pricing below)
#
# === IMAGE GENERATION MODELS (Very Cheap: < 0.001/image) ===
#   - flux: Flux Schnell - High-quality image generation (0.0002/image)
#   - zimage: Z-Image Turbo - Fast image generation (0.0002/image)
#   - turbo: SDXL Turbo - Ultra-fast image generation (0.0003/image)
#
# === TEXT GENERATION MODELS (Very Cheap: < 0.5/M input) ===
#   - nova-fast: Amazon Nova Micro - Fast text generation (0.04/M in, 0.15/M out)
#   - qwen-coder: Qwen3 Coder 30B - Code generation (0.06/M in, 0.22/M out)
#   - gemini-fast: Google Gemini 2.5 Flash Lite - Fast with search (0.1/M in, 0.4/M out)
#   - mistral: Mistral Small 3.2 24B - Balanced performance (0.15/M in, 0.35/M out)
#   - grok: xAI Grok 4 Fast - Fast reasoning (0.2/M in, 0.5/M out)
#   - openai-fast: OpenAI GPT-5 Nano - Ultra-fast (0.06/M in, 0.44/M out)
#
# === VIDEO GENERATION MODELS (Very Cheap: < 0.03/sec) ===
#   - wan: Wan 2.6 - Fast video generation (0.025/sec)
#
# Configuration Notes:
#   - API_KEY: Optional but recommended for better performance
#   - ENABLED: Enable/disable Pollinations integration
#   - MODEL: Default model (use 'flux' for images, 'nova-fast' for text)
POLLINATIONS_API_KEY=your_pollinations_api_key_here
POLLINATIONS_ENABLED=true
POLLINATIONS_MODEL=flux

# Server Configuration
PORT=3000
HOST=localhost

# Plugin Configuration
PLUGINS_DIR=./src/plugins
INTEGRATIONS_DIR=./src/integrations

# Logging
LOG_LEVEL=info
LOG_FILE=./logs/assistant.log

# Security
API_SECRET_KEY=your_secret_key_here
ENABLE_AUTH=false

# API Key Encryption (for storing API keys securely)
ENCRYPTION_KEY=your_32_character_encryption_key_here

# Browser Automation (for web integrations)
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000

# File System
ALLOWED_DIRECTORIES=./workspace
MAX_FILE_SIZE=10485760

# Rate Limiting (Global)
# Note: Provider-specific rate limits (e.g., GEMINI_MAX_REQUESTS_PER_MINUTE) 
# take precedence over this global setting for their respective providers.
# This global limit applies to providers without specific rate limit configuration.
MAX_REQUESTS_PER_MINUTE=60
